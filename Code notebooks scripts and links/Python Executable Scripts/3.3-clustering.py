# -*- coding: utf-8 -*-
"""3.3-Clustering

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/nidalshahin/3-3-clustering.75fe3e6d-1c03-4ffc-804c-9dee22bda352.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250517/auto/storage/goog4_request%26X-Goog-Date%3D20250517T102135Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9dcb5b0745ab3b34adca727ecf0bbd341bd2e84949de5c6eb5520bf9051d63502875ed708df90cc0acb6a21cc835e7c6c7f78204de1e0ffd17958c952046e08d6b8d584101898aaf7517653cdf51f921956c3f77b16b14abcd3680054f2f1334eb4e637d8b51e55698bdd247fd477495eb4927da5edabb1ded3cd62270cbd91db8eb89f6e899eba439a61b9e6e88bfbe212c7aad9a5a2d6521506d0cfbb3c1ce10f48e1181d0c30a699ce1e066044d960708e8c6679a63021aa6b95e0639a908e635acf69f2dd92b39d2a56f899e44efb79d96e689bb55550e94985c5beff6d9f035d30885b33c3e7ef2c9090fdb9953d8e947e50d2a8b3bb9a634e2b1791644

# 3.3 Clustering: Unsupervised Job Grouping with RAPIDS cuML
In this notebook we cluster job postings using feature embeddings and analyze the clusters using job titles.

## RAPIDS Environment Assertion
"""

import os
assert os.environ.get("RAPIDS_NO_INITIALIZE") is None, "Make sure you're using the RAPIDS Kaggle Docker image, GPU T4 as accelerator"
!nvidia-smi

"""## Libraries Required"""

import gc
import joblib
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.sparse import load_npz
from mpl_toolkits.mplot3d import Axes3D

from cuml.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import davies_bouldin_score

"""## Data Loading"""

test_df = pd.read_parquet('/kaggle/input/ds-preprocessing-output-files/test_data.parquet')
X_test_emb  = np.load('/kaggle/input/ds-preprocessing-output-files/fused_emb_test.npy')
X_test_skills = load_npz('/kaggle/input/ds-preprocessing-output-files/skills_tfidf_test.npz').toarray()

job_title_encoder = joblib.load('/kaggle/input/ds-preprocessing-output-files/label_encoder_job_title (1).pkl')
job_titles = job_title_encoder.inverse_transform(test_df['Job Title_encoded'].values)
test_df['job_title'] = job_titles

print('> Data loaded successfully')

"""## Clustering with cuML KMeans
- We'll use KMeans for our clustering task. Number of clusters (k) can vary based on the users needs.
"""

# Feature sets and cluster numbers to loop through
feature_sets = [
    {'name': 'Skills TF-IDF', 'data': X_test_skills, 'type': 'tfidf'},
    {'name': 'Role SBERT', 'data': X_test_emb, 'type': 'sbert'}
]

cluster_numbers = [10, 25, 40]

# Looping through all feature-cluster combinations
for feature in feature_sets:
    feature_name = feature['name']
    X_cluster = feature['data']
    feat_type = feature['type']

    for K in cluster_numbers:
        print(f"\n{'='*80}")
        print(f"Clustering Analysis based on: {feature_name} with K={K}")
        print(f"{'='*80}\n")

        # 1) Fit the data
        print("1) Fitting KMeans...")
        kmeans = KMeans(n_clusters=K, random_state=42)
        clusters = kmeans.fit_predict(X_cluster)
        test_df['cluster'] = clusters  # Added to dataframe for analysis
        print(f'KMeans clustering complete. Number of clusters: {K}\n')

        # 2) PCA Visualization (3D)
        print("2) Creating 3D PCA visualization...")
        pca = PCA(n_components=3, random_state=42)
        X_pca = pca.fit_transform(X_cluster)

        fig = plt.figure(figsize=(12, 8))
        ax = fig.add_subplot(111, projection='3d')

        scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],
                             c=clusters, cmap='tab20b', alpha=0.6,
                             s=30, edgecolor='k', linewidth=0.2)

        ax.set_title(f'3D PCA Projection\n{feature_name}, K={K}', pad=20)
        ax.set_xlabel('Principal Component 1')
        ax.set_ylabel('Principal Component 2')
        ax.set_zlabel('Principal Component 3')

        fig.colorbar(scatter, label='Cluster', boundaries=np.arange(K+1)-0.5).set_ticks(np.arange(K))

        plt.grid(alpha=0.2)
        plt.tight_layout()
        plt.show()

        # 3) Clustering Metrics
        print("3) Computing Davies Bouldin Score [0,âˆž) -The closer to 0 the better- ...")
        db_score = davies_bouldin_score(X_cluster, clusters)
        print(f"Davies-Bouldin Score: {db_score:.4f}\n")

        # 4) Top Job Titles per Cluster
        print("4) Analyzing top job titles per cluster...")
        k_top = 5
        cluster_job_counts = (
            test_df.groupby('cluster')['job_title']
            .value_counts()
            .groupby(level=0)
            .head(k_top)
            .unstack()
            .fillna('')
        )

        print(f'Top {k_top} job titles per cluster:')
        display(cluster_job_counts)

        # 5) Cluster Distribution for Top Jobs
        print("\n5) Creating cluster distribution plot...")
        top_n_jobs = 10
        top_jobs = test_df['job_title'].value_counts().head(top_n_jobs).index

        plt.figure(figsize=(14, 6))
        ax = sns.countplot(data=test_df[test_df['job_title'].isin(top_jobs)],
                          x='job_title', hue='cluster',
                          palette='tab20b', hue_order=np.arange(K))
        plt.title(f'Cluster Distribution for Top {top_n_jobs} Job Titles\n{feature_name}, K={K}', pad=20)
        plt.xlabel('Job Title')
        plt.ylabel('Count')
        plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.xticks(rotation=45, ha='right')
        plt.grid(axis='y', alpha=0.2)
        plt.tight_layout()
        plt.show()

        # 6) Cluster Sizes
        print("6) Analyzing cluster sizes...")
        plt.figure(figsize=(12, 4))
        ax = sns.countplot(x='cluster', data=test_df,
                          palette='tab20b', order=np.arange(K))
        plt.title(f'Cluster Sizes Distribution\n{feature_name}, K={K}', pad=20)
        plt.xlabel('Cluster ID')
        plt.ylabel('Number of Samples')
        plt.xticks(rotation=0)
        plt.grid(axis='y', alpha=0.2)

        # Adding counts on top of bars
        for p in ax.patches:
            ax.annotate(f'{p.get_height():.0f}',
                        (p.get_x() + p.get_width() / 2., p.get_height()),
                        ha='center', va='center',
                        xytext=(0, 5), textcoords='offset points')

        plt.tight_layout()
        plt.show()

        # 7) Cluster Centers Analysis
        print("\n7) Analyzing cluster centers...")
        centers = kmeans.cluster_centers_

        if feat_type == 'tfidf':
            print("\nTop features per cluster:")
            feature_names = [f'tfidf_{i}' for i in range(X_cluster.shape[1])]
            for i, center in enumerate(centers):
                top_feat_idx = np.argsort(center)[-5:][::-1]
                print(f'Cluster {i}: {[feature_names[j] for j in top_feat_idx]}')

        elif feat_type == 'sbert':
            print("\nSBERT cluster centers (first 5 dimensions):")
            for i, center in enumerate(centers):
                print(f'Cluster {i}: {center[:5].round(4)}...')

        print(f"\n{'='*80}")
        print(f"COMPLETED Analysis based on: {feature_name} with K={K}")
        print(f"{'='*80}\n\n")