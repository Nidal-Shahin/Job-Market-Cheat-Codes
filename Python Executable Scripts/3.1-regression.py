# -*- coding: utf-8 -*-
"""3.1-Regression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/nidalshahin/3-1-regression.b915b5b3-f8d6-4285-8cec-2c730ecc91d5.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250516/auto/storage/goog4_request%26X-Goog-Date%3D20250516T201452Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D16a106ce53632a6c5b356be9cba7f8ee60e8fb7c2bcec5d3343bddf7b59b9938671bc49893b79ed7eef2e27a232a3973a8413bab77620649978867e24f29e3a96a54ed67d2b74a86396972aecf0ab615ca1cb139d6c87c423c987c7f6e630bce825286fa651244581768ccbc48f9a9bb36bc262198c319f82f3da4c91ece992bfc8b50aea39ad606283de7e773449ceadd5e422e19d43401f61406932b5e96e35f2ed6c47ae07e10ae8436e8c7f00f89f1e4196417b9582ae9b2014dbb8a601f65159ee977c5deea44205815f0f1fe7488392ec16ca2f5c772dad7a292b054662b43eb714a76c5c9f86fc699e5ae9b9fa2d8fc296020c8cc48fc747fa19ec661

# 3.1 Regression: Salary Prediction with RAPIDS cuML

This notebook predicts job salary using various feature sets and RAPIDS cuML models.

## RAPIDS Environment Assertion
"""

import os
assert os.environ.get("RAPIDS_NO_INITIALIZE") is None, "Make sure you're using the RAPIDS Kaggle Docker image, GPU T4 as accelerator"
!nvidia-smi

"""## Libraries Required"""

import joblib
import numpy as np
import pandas as pd
import gc
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.sparse import load_npz, hstack

from cuml.linear_model import Ridge, Lasso
from cuml.metrics import mean_squared_error
from cuml.neighbors import KNeighborsRegressor
from cuml.svm import SVR

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import permutation_importance

"""## Data Loading"""

# Data Loading
train_df = pd.read_parquet('/kaggle/input/ds-preprocessing-output-files/train_data.parquet')
dev_df   = pd.read_parquet('/kaggle/input/ds-preprocessing-output-files/dev_data.parquet')
test_df  = pd.read_parquet('/kaggle/input/ds-preprocessing-output-files/test_data.parquet')

X_train_emb = np.load('/kaggle/input/ds-preprocessing-output-files/fused_emb_train.npy')
X_dev_emb   = np.load('/kaggle/input/ds-preprocessing-output-files/fused_emb_dev.npy')
X_test_emb  = np.load('/kaggle/input/ds-preprocessing-output-files/fused_emb_test.npy')

X_train_skills = load_npz('/kaggle/input/ds-preprocessing-output-files/skills_tfidf_train.npz')
X_dev_skills   = load_npz('/kaggle/input/ds-preprocessing-output-files/skills_tfidf_dev.npz')
X_test_skills  = load_npz('/kaggle/input/ds-preprocessing-output-files/skills_tfidf_test.npz')

y_train = train_df['salary_avg'].values
y_dev   = dev_df['salary_avg'].values
y_test  = test_df['salary_avg'].values

drop_cols = ['Job Title_encoded', 'salary_avg']
X_train_struct = train_df.drop(columns=drop_cols).values
X_dev_struct   = dev_df.drop(columns=drop_cols).values
X_test_struct  = test_df.drop(columns=drop_cols).values

del dev_df, test_df
gc.collect();

"""## Ridge Regression - Feature Importance for Structured Features

- We will use cuML's Ridge regression coefficients as a proxy for feature importance.
"""

# Ridge Regression for feature importance
ridge = Ridge(alpha=1.0)
ridge.fit(X_train_struct, y_train)
coefs = ridge.coef_.get() if hasattr(ridge.coef_, 'get') else ridge.coef_

feature_names = train_df.drop(columns=drop_cols).columns
feat_imp = pd.Series(np.abs(coefs), index=feature_names).sort_values(ascending=False)
plt.figure(figsize=(10,6))
sns.barplot(x=feat_imp.values[:15], y=feat_imp.index[:15])
plt.title('Top 15 Structured Feature Importances (Ridge)')
plt.tight_layout()
plt.show()

feat_imp

"""## Trying Different Feature (X) Combinations

- We'll compare Ridge model trained on: structured only, embeddings only, skills only, and a mix of them.
"""

combos = {
    'structured': X_train_struct,
    'fused SBERT embeddings': X_train_emb,
    'tfidf embeddings': X_train_skills.toarray(),
    'str+emb': np.hstack([X_train_struct, X_train_skills.toarray()])
}
dev_combos = {
    'structured': X_dev_struct,
    'fused SBERT embeddings': X_dev_emb,
    'tfidf embeddings': X_dev_skills.toarray(),
    'str+emb': np.hstack([X_dev_struct, X_dev_skills.toarray()])
}
results = {}
for name, Xtr in combos.items():
    print(f'Training Ridge Regression on {name} features...')
    model = Ridge(alpha=0.1)
    model.fit(Xtr, y_train)
    y_pred = model.predict(dev_combos[name])
    mse = mean_squared_error(y_dev, y_pred)
    rmse = np.sqrt(mse)
    y_min, y_max = np.min(y_dev), np.max(y_dev)
    normalized_rmse = rmse / (y_max - y_min)
    results[name] = normalized_rmse.astype(np.float16)
    print(f'RMSE: {normalized_rmse:.2f}\t range:[0-1] closer to 0 the better. \n')
    del Xtr, model
    gc.collect();

results

"""## Final Ridge Model: Tuning and Evaluating on Test Set

We'll use the best feature combo and tune the model.
"""

best_combo = min(results, key=results.get)
print(f'Best feature set: {best_combo}')
Xtr = combos[best_combo]
Xdev = dev_combos[best_combo]
Xte = np.hstack([X_test_struct, X_test_emb, X_test_skills.toarray()]) if best_combo == 'str+emb' else (
    X_test_struct if best_combo == 'structured' else (
        X_test_emb if best_combo == 'fused SBERT embeddings' else X_test_skills.toarray()
    )
)

# Hyperparameter tuning (alpha)
best_rmse = float('inf')
best_alpha = 1.0
for alpha in [0.1, 1.0, 10.0, 100.0]:
    model = Ridge(alpha=alpha)
    model.fit(Xtr, y_train)
    y_pred = model.predict(Xdev)
    rmse = np.sqrt(mean_squared_error(y_dev, y_pred))
    if rmse < best_rmse:
        best_rmse = rmse
        best_alpha = alpha
print(f'Best alpha: {best_alpha}, Dev RMSE: {best_rmse:.2f}')

# Train on train+dev, evaluate on test
X_train_full = np.vstack([Xtr, Xdev])
y_train_full = np.concatenate([y_train, y_dev])
model = Ridge(alpha=best_alpha)
model.fit(X_train_full, y_train_full)
y_test_pred = model.predict(Xte)
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
print(f'Test RMSE: {test_rmse:.2f}')

# Save model
joblib.dump(model, '/kaggle/working/ridge_regression_cuml.joblib')

"""## Visual: True vs. Predicted Salary (Test Set)"""

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_test_pred, alpha=0.2, edgecolors='k')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual Salary')
plt.ylabel('Predicted Salary')
plt.title('Salary Prediction - Actual vs. Predicted (Test Set)')
plt.grid(True)
plt.tight_layout()
plt.show()

"""## 2nd Model: K Neighbors Regressor

### Training then Evaluating on Dev Set
"""

model = KNeighborsRegressor(n_neighbors=5)
model.fit(Xtr, y_train)
y_pred = model.predict(Xdev)
rmse = np.sqrt(mean_squared_error(y_dev, y_pred))
y_min, y_max = np.min(y_dev), np.max(y_dev)
normalized_rmse = rmse / (y_max - y_min)
print(f'KNN RMSE: {rmse:.4f}, Normalized RMSE [0-1]: {normalized_rmse:.4f}')

"""### Visual: True vs. Predicted Salary (Dev Set)"""

plt.figure(figsize=(8, 6))
plt.scatter(y_dev, y_pred, alpha=0.2, edgecolors='k')
plt.plot([y_dev.min(), y_dev.max()], [y_dev.min(), y_dev.max()], 'r--')
plt.xlabel('Actual Salary')
plt.ylabel('Predicted Salary')
plt.title('Salary Prediction - Actual vs. Predicted (Dev Set)')
plt.grid(True)
plt.tight_layout()
plt.show()

"""### Visual: Permutation Importance"""

idx = np.random.choice(len(y_dev), size=1000, replace=False)
Xdev_sub = Xdev[idx]
y_dev_sub = y_dev[idx]

result = permutation_importance(
    model, Xdev_sub, y_dev_sub, n_repeats=3, random_state=42, scoring='neg_root_mean_squared_error'
)

importances = pd.Series(result.importances_mean, index=feature_names)
importances = importances.sort_values(ascending=False)

plt.figure(figsize=(10,6))
importances[:15].plot(kind='barh')
plt.title('Top 15 Feature Importances (Permutation, KNN)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

importances

"""## 3rd Model: SVM Regressor (SVR)

### Training then Evaluating on Dev Set
"""

model = SVR(C=1.0, kernel='rbf')
model.fit(Xtr, y_train)
y_pred = model.predict(Xdev)
rmse = np.sqrt(mean_squared_error(y_dev, y_pred))
y_min, y_max = np.min(y_dev), np.max(y_dev)
normalized_rmse = rmse / (y_max - y_min)
print(f'SVR RMSE: {rmse:.4f}, Normalized RMSE [0-1]: {normalized_rmse:.4f}')

"""### Visual: True vs. Predicted Salary (Dev Set)"""

plt.figure(figsize=(8, 6))
plt.scatter(y_dev, y_pred, alpha=0.2, edgecolors='k')
plt.plot([y_dev.min(), y_dev.max()], [y_dev.min(), y_dev.max()], 'r--')
plt.xlabel('Actual Salary')
plt.ylabel('Predicted Salary')
plt.title('Salary Prediction - Actual vs. Predicted (Dev Set)')
plt.grid(True)
plt.tight_layout()
plt.show()

"""### Visual: Permutation Importance"""

result = permutation_importance(
    model, Xdev_sub, y_dev_sub, n_repeats=3, random_state=42, scoring='neg_root_mean_squared_error'
)

# Display top features
importances = pd.Series(result.importances_mean, index=feature_names)
importances = importances.sort_values(ascending=False)

plt.figure(figsize=(10,6))
importances[:15].plot(kind='barh')
plt.title('Top 15 Feature Importances (Permutation, SVR)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

importances